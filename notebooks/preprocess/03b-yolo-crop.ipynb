{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a42c7-ad81-4cc0-83b8-fa8a87e978b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Config\n",
    "# --- YOLOv8 ONNX Preview: Pixel-perfect crops matching drawn bbox ---\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Iterable\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc18d5-0a67-4adb-88ce-d7ccd44db9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PreviewConfig:\n",
    "    input_dir: Path = Path(\"/workspace/\")\n",
    "    model_path: Path = Path(\"/workspace/runs/detect/train/weights/best.onnx\")\n",
    "\n",
    "    # Detection filtering\n",
    "    target_classes: Optional[set[str]] = frozenset({\"target\"})  # None => no filtering\n",
    "    conf_thres: float = 0.10\n",
    "    iou_thres: float = 0.45\n",
    "\n",
    "    # Preview/cropping behavior\n",
    "    take_largest_box: bool = True\n",
    "    pad_frac: float = 0.0                  # expand bbox by pad_frac * max(w, h)\n",
    "    preview_images: int = 20               # number of images to preview\n",
    "    max_previews_per_image: int = 3        # max crops to show per image\n",
    "\n",
    "    # Sampling (optional)\n",
    "    random_sample: bool = False\n",
    "    seed: int = 42\n",
    "\n",
    "CFG = PreviewConfig()\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e868086-57cf-4bf9-9369-6a0dc493482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities (RGB conversion, bbox integerization, drawing)\n",
    "\n",
    "SUPPORTED_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "\n",
    "def to_rgb(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert BGR/BGRA image (OpenCV) to RGB/RGBA for matplotlib.\"\"\"\n",
    "    if img is None:\n",
    "        return None\n",
    "    if img.ndim == 3 and img.shape[2] == 4:\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGRA2RGBA)\n",
    "    if img.ndim == 3 and img.shape[2] == 3:\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_image_any(path: Path) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load image with alpha if present.\"\"\"\n",
    "    img = cv2.imread(str(path), cv2.IMREAD_UNCHANGED)\n",
    "    return img\n",
    "\n",
    "\n",
    "def bbox_to_int_exclusive(\n",
    "    xyxy: np.ndarray,\n",
    "    img_shape: tuple,\n",
    "    pad_frac: float = 0.0,\n",
    "    rounding: str = \"round\",\n",
    ") -> tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Convert float xyxy to integer bbox with end-exclusive convention:\n",
    "      crop = img[y1:y2, x1:x2]\n",
    "    This makes bbox math consistent with numpy slicing.\n",
    "\n",
    "    rounding:\n",
    "      - 'round'     : round all coords\n",
    "      - 'floorceil' : floor for (x1,y1), ceil for (x2,y2)\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = map(float, xyxy)\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    pad = pad_frac * max(w, h)\n",
    "\n",
    "    x1 -= pad; y1 -= pad; x2 += pad; y2 += pad\n",
    "\n",
    "    if rounding == \"round\":\n",
    "        xi1 = int(np.round(x1)); yi1 = int(np.round(y1))\n",
    "        xi2 = int(np.round(x2)); yi2 = int(np.round(y2))\n",
    "    else:\n",
    "        xi1 = int(np.floor(x1)); yi1 = int(np.floor(y1))\n",
    "        xi2 = int(np.ceil(x2));  yi2 = int(np.ceil(y2))\n",
    "\n",
    "    H, W = img_shape[:2]\n",
    "\n",
    "    # Clamp to [0..W] / [0..H] for end-exclusive upper bound\n",
    "    xi1 = max(0, min(W, xi1))\n",
    "    yi1 = max(0, min(H, yi1))\n",
    "    xi2 = max(0, min(W, xi2))\n",
    "    yi2 = max(0, min(H, yi2))\n",
    "\n",
    "    # Ensure non-empty region for slicing\n",
    "    if xi2 <= xi1: xi2 = min(W, xi1 + 1)\n",
    "    if yi2 <= yi1: yi2 = min(H, yi1 + 1)\n",
    "\n",
    "    return xi1, yi1, xi2, yi2\n",
    "\n",
    "\n",
    "def draw_boxes_pixelperfect(\n",
    "    img: np.ndarray,\n",
    "    boxes_xyxy: np.ndarray,\n",
    "    clses: np.ndarray,\n",
    "    confs: np.ndarray,\n",
    "    id2name: dict,\n",
    "    pad_frac: float = 0.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw rectangles consistent with numpy cropping.\n",
    "    Because cv2.rectangle uses inclusive endpoint, we draw at (x2-1, y2-1)\n",
    "    for end-exclusive bbox [x1, x2), [y1, y2).\n",
    "    \"\"\"\n",
    "    vis = to_rgb(img).copy()\n",
    "\n",
    "    # For cv2 drawing, we need a BGR/RGB image. We'll draw on BGR then convert back.\n",
    "    # If vis is RGBA, convert to BGR (alpha discarded for drawing) then back to RGB.\n",
    "    if vis.ndim == 3 and vis.shape[2] == 4:\n",
    "        draw_img = cv2.cvtColor(vis, cv2.COLOR_RGBA2BGR)\n",
    "        back_to_rgb = lambda x: cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        draw_img = cv2.cvtColor(vis, cv2.COLOR_RGB2BGR)\n",
    "        back_to_rgb = lambda x: cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    for (x1, y1, x2, y2), c, cf in zip(boxes_xyxy, clses, confs):\n",
    "        xi1, yi1, xi2, yi2 = bbox_to_int_exclusive(\n",
    "            (x1, y1, x2, y2), draw_img.shape, pad_frac=pad_frac, rounding=\"round\"\n",
    "        )\n",
    "\n",
    "        # pixel-perfect: draw inclusive endpoint to match slicing\n",
    "        pt1 = (xi1, yi1)\n",
    "        pt2 = (max(xi1, xi2 - 1), max(yi1, yi2 - 1))\n",
    "\n",
    "        cv2.rectangle(draw_img, pt1, pt2, (0, 255, 0), 2)\n",
    "\n",
    "        label = f\"{id2name.get(int(c), int(c))}:{float(cf):.2f}\"\n",
    "        cv2.putText(\n",
    "            draw_img, label, (xi1, max(0, yi1 - 5)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "    return back_to_rgb(draw_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747700c-41cb-4d03-b76d-7af2b647286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX input-size detection (imgsz) + Provider check\n",
    "\n",
    "def detect_onnx_input_hw(onnx_path: Path, fallback: int = 512) -> int:\n",
    "    \"\"\"\n",
    "    Read expected HxW from ONNX input shape to avoid size mismatch errors.\n",
    "    Returns a single imgsz (min(H,W) if not square).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import onnxruntime as ort\n",
    "        sess = ort.InferenceSession(\n",
    "            str(onnx_path),\n",
    "            providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "        )\n",
    "        shp = sess.get_inputs()[0].shape  # e.g., [None, 3, 512, 512]\n",
    "        H = shp[2] if isinstance(shp[2], int) else fallback\n",
    "        W = shp[3] if isinstance(shp[3], int) else fallback\n",
    "        return int(min(H, W)) if H != W else int(H)\n",
    "    except Exception:\n",
    "        return int(fallback)\n",
    "\n",
    "\n",
    "# Provider availability info (optional)\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    providers = ort.get_available_providers()\n",
    "    print(\"[INFO] ONNXRuntime providers:\", providers)\n",
    "    if \"CUDAExecutionProvider\" not in providers:\n",
    "        print(\"[INFO] CUDAExecutionProvider not available -> inference will run on CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"[INFO] ONNXRuntime check skipped: {e}\")\n",
    "\n",
    "imgsz = detect_onnx_input_hw(CFG.model_path, fallback=512)\n",
    "print(f\"[INFO] Detected ONNX imgsz: {imgsz}x{imgsz}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b35043-55b8-414d-97a9-70692d12c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO ONNX + Resolve classes safely\n",
    "\n",
    "model = YOLO(str(CFG.model_path))\n",
    "\n",
    "# Safe names extraction (ONNX sometimes needs fallback)\n",
    "id2name = (\n",
    "    getattr(getattr(model, \"model\", None), \"names\", None)\n",
    "    or getattr(model, \"names\", None)\n",
    "    or {0: \"class0\"}\n",
    ")\n",
    "\n",
    "# Normalize name->id (case-insensitive)\n",
    "name2id_lower = {str(v).lower(): int(k) for k, v in id2name.items()}\n",
    "\n",
    "if CFG.target_classes is None:\n",
    "    target_ids = None\n",
    "else:\n",
    "    missing = [c for c in CFG.target_classes if c.lower() not in name2id_lower]\n",
    "    if missing:\n",
    "        print(f\"[WARN] Missing classes in model: {missing} -> disable class filtering (show all detections).\")\n",
    "        target_ids = None\n",
    "    else:\n",
    "        target_ids = {name2id_lower[c.lower()] for c in CFG.target_classes}\n",
    "\n",
    "print(\"[INFO] model.names:\", id2name)\n",
    "print(\"[INFO] target_ids :\", target_ids if target_ids is not None else \"ALL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4adf0cf-cd61-43c0-b89d-91d7136cdb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect images + Run prediction + Build previews\n",
    "\n",
    "def list_images(input_dir: Path) -> list[Path]:\n",
    "    files = []\n",
    "    for p in input_dir.iterdir():\n",
    "        if p.is_file() and p.suffix.lower() in SUPPORTED_EXTS:\n",
    "            files.append(p)\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "\n",
    "img_files = list_images(CFG.input_dir)\n",
    "if not img_files:\n",
    "    raise RuntimeError(f\"No images found in: {CFG.input_dir}\")\n",
    "\n",
    "# Optionally random sample\n",
    "if CFG.random_sample:\n",
    "    rng = np.random.default_rng(CFG.seed)\n",
    "    idx = rng.choice(len(img_files), size=min(CFG.preview_images, len(img_files)), replace=False)\n",
    "    img_files = [img_files[i] for i in sorted(idx)]\n",
    "else:\n",
    "    img_files = img_files[:CFG.preview_images]\n",
    "\n",
    "print(f\"[INFO] Previewing {len(img_files)} image(s).\")\n",
    "\n",
    "rows = []\n",
    "for path in img_files:\n",
    "    img = load_image_any(path)\n",
    "    if img is None:\n",
    "        rows.append((path.name, None, []))\n",
    "        continue\n",
    "\n",
    "    # NOTE:\n",
    "    # For ONNX, actual provider selection is handled by onnxruntime.\n",
    "    # Ultralytics 'device' arg may not force CUDA for ONNX the same way as PyTorch.\n",
    "    results = model.predict(\n",
    "        source=str(path),\n",
    "        imgsz=imgsz,\n",
    "        conf=CFG.conf_thres,\n",
    "        iou=CFG.iou_thres,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        rows.append((path.name, to_rgb(img), []))\n",
    "        continue\n",
    "\n",
    "    res = results[0]\n",
    "    if res.boxes is None or len(res.boxes) == 0:\n",
    "        rows.append((path.name, to_rgb(img), []))\n",
    "        continue\n",
    "\n",
    "    boxes = res.boxes.xyxy.detach().cpu().numpy()\n",
    "    clses = res.boxes.cls.detach().cpu().numpy().astype(int)\n",
    "    confs = res.boxes.conf.detach().cpu().numpy()\n",
    "\n",
    "    # Visualization: draw all detections\n",
    "    vis = draw_boxes_pixelperfect(img, boxes, clses, confs, id2name, pad_frac=CFG.pad_frac)\n",
    "\n",
    "    # Filter class if requested\n",
    "    idxs = list(range(len(clses)))\n",
    "    if target_ids is not None:\n",
    "        idxs = [i for i in idxs if clses[i] in target_ids]\n",
    "\n",
    "    # If empty after filtering, fallback to top-1 confidence\n",
    "    if not idxs:\n",
    "        idxs = [int(np.argmax(confs))]\n",
    "\n",
    "    # Choose largest box or top-N boxes\n",
    "    if CFG.take_largest_box and len(idxs) > 1:\n",
    "        areas = [(i, (boxes[i][2] - boxes[i][0]) * (boxes[i][3] - boxes[i][1])) for i in idxs]\n",
    "        areas.sort(key=lambda x: x[1], reverse=True)\n",
    "        idxs = [areas[0][0]]\n",
    "    else:\n",
    "        idxs = idxs[:CFG.max_previews_per_image]\n",
    "\n",
    "    crops = []\n",
    "    for i in idxs:\n",
    "        xi1, yi1, xi2, yi2 = bbox_to_int_exclusive(boxes[i], img.shape, pad_frac=CFG.pad_frac, rounding=\"round\")\n",
    "        crop = img[yi1:yi2, xi1:xi2]  # end-exclusive slicing\n",
    "        crops.append((id2name.get(int(clses[i]), f\"cls{int(clses[i])}\"), float(confs[i]), to_rgb(crop)))\n",
    "\n",
    "    rows.append((path.name, vis, crops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25909b-52cb-42cd-9852-372bfddb1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display previews\n",
    "for fname, vis, crops in rows:\n",
    "    if vis is None:\n",
    "        print(f\"[SKIP] failed to load: {fname}\")\n",
    "        continue\n",
    "\n",
    "    cols = 1 + max(1, len(crops))\n",
    "    plt.figure(figsize=(5 * cols, 5))\n",
    "\n",
    "    plt.subplot(1, cols, 1)\n",
    "    plt.imshow(vis)\n",
    "    plt.title(f\"Detections: {fname}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if crops:\n",
    "        for j, (cls_name, cf, cr) in enumerate(crops, start=2):\n",
    "            plt.subplot(1, cols, j)\n",
    "            plt.imshow(cr)\n",
    "            plt.title(f\"Crop ({cls_name}, conf={cf:.2f})\")\n",
    "            plt.axis(\"off\")\n",
    "    else:\n",
    "        plt.subplot(1, cols, 2)\n",
    "        plt.imshow(vis)\n",
    "        plt.title(\"No crops (after filter)\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
