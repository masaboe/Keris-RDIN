{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Notebook â€” CLIP ViT-B (Baseline)\n",
    "\n",
    "This notebook trains and evaluates **CLIP ViT-B (Baseline)** for the Keris image classification task.  \n",
    "It has been refactored for **reproducibility** and to serve as a clean **appendix artifact** for journal submission.\n",
    "\n",
    "## Recommended folder conventions\n",
    "- **Input data**: keep dataset paths configurable (see the *Configuration* cell).\n",
    "- **Outputs / artifacts**: write all run artifacts under `artifacts/06_clip_vit_b/` (created automatically below).\n",
    "\n",
    "## Reproducibility checklist\n",
    "- Fixed random seed (NumPy / framework seed)\n",
    "- Best-effort deterministic operations (may vary by GPU/driver)\n",
    "- Logged environment versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment & reproducibility (PyTorch) ---\n",
    "import os, sys, platform, random\n",
    "import numpy as np\n",
    "\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"Python   :\", sys.version.split()[0])\n",
    "print(\"Platform :\", platform.platform())\n",
    "print(\"NumPy    :\", np.__version__)\n",
    "print(\"Torch    :\", torch.__version__)\n",
    "print(\"CUDA     :\", torch.version.cuda)\n",
    "print(\"Device   :\", \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (paths & artifact directory) ---\n",
    "from pathlib import Path\n",
    "\n",
    "# Project root: by default, current working directory\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Edit these paths if needed\n",
    "DATA_ROOT = PROJECT_ROOT / \"dataset\"      # <-- set your dataset root here\n",
    "NPY_ROOT  = PROJECT_ROOT / \"npy\"          # <-- set your .npy root here (if used)\n",
    "\n",
    "# All outputs should go here\n",
    "ARTIFACT_DIR = PROJECT_ROOT / \"artifacts\" / \"06_clip_vit_b\"\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT :\", PROJECT_ROOT)\n",
    "print(\"DATA_ROOT    :\", DATA_ROOT)\n",
    "print(\"NPY_ROOT     :\", NPY_ROOT)\n",
    "print(\"ARTIFACT_DIR :\", ARTIFACT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & evaluation (original workflow)\n",
    "The cells below contain the original training pipeline with minimal functional changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xi2Otk7BnDH-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, site\n",
    "print(\"python exe:\", sys.executable)\n",
    "print(\"site-packages:\", site.getsitepackages()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"torch file:\", torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.utils import is_torch_available, is_tf_available\n",
    "\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"is_torch_available:\", is_torch_available())\n",
    "print(\"is_tf_available   :\", is_tf_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torch cuda build:\", torch.version.cuda)\n",
    "print(\"is_available:\", torch.cuda.is_available())\n",
    "print(\"device_count:\", torch.cuda.device_count())\n",
    "\n",
    "# paksa init\n",
    "torch.cuda.init()\n",
    "print(\"GPU0:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcCX_UhLnFUV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "# import keras_hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical # convert to one-hot-encoding\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "X_train= \"npyBilah/aug/x_train_aug.npy\"\n",
    "X_test = \"npyBilah/x_test.npy\"\n",
    "X_val = \"npyBilah/x_valid.npy\"\n",
    "y_train= \"npyBilah/aug/y_train_aug.npy\"\n",
    "y_test = \"npyBilah/y_test.npy\"\n",
    "y_val = \"npyBilah/y_valid.npy\"\n",
    "X_train = np.load(X_train)\n",
    "X_test = np.load(X_test)\n",
    "X_val = np.load(X_val)\n",
    "y_train = np.load(y_train)\n",
    "y_test = np.load(y_test)\n",
    "y_val = np.load(y_val)\n",
    "# seed_everything(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hitung jumlah class\n",
    "classes, counts = np.unique(y_train, axis=0, return_counts=True)\n",
    "\n",
    "# Print hasilnya\n",
    "print(\"Kelas: \", classes)\n",
    "print(\"Jumlah: \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hitung jumlah class\n",
    "classes1, counts1 = np.unique(y_test, axis=0, return_counts=True)\n",
    "\n",
    "# Print hasilnya\n",
    "print(\"Kelas: \", classes1)\n",
    "print(\"Jumlah: \", counts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hitung jumlah class\n",
    "classes2, counts2 = np.unique(y_val, axis=0, return_counts=True)\n",
    "\n",
    "# Print hasilnya\n",
    "print(\"Kelas: \", classes2)\n",
    "print(\"Jumlah: \", counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts, counts1, counts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "num_classes = y_train.shape[1]\n",
    "print(\"num_classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyCLIPDataset(Dataset):\n",
    "    def __init__(self, X, y_onehot, processor):\n",
    "        self.X = X\n",
    "        self.y = y_onehot\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]  # (H,W,3) float32 [0,1]\n",
    "        img_u8 = (np.clip(img, 0, 1) * 255.0).astype(np.uint8)\n",
    "\n",
    "        proc = self.processor(images=img_u8, return_tensors=\"pt\")\n",
    "        pixel_values = proc[\"pixel_values\"].squeeze(0)  # (3,224,224)\n",
    "\n",
    "        y_idx = int(np.argmax(self.y[idx]))  # one-hot -> index\n",
    "        return pixel_values, y_idx\n",
    "\n",
    "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "train_ds = NumpyCLIPDataset(X_train, y_train, processor)\n",
    "val_ds   = NumpyCLIPDataset(X_val,   y_val,   processor)\n",
    "test_ds  = NumpyCLIPDataset(X_test,  y_test,  processor)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_labels = np.argmax(y_train, axis=1)\n",
    "classes = np.unique(y_labels)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_labels)\n",
    "\n",
    "class_weights_t = torch.ones(num_classes, dtype=torch.float32)\n",
    "for c, w in zip(classes, cw):\n",
    "    class_weights_t[int(c)] = float(w)\n",
    "class_weights_t = class_weights_t.to(device)\n",
    "\n",
    "print(\"class_weights:\", class_weights_t)\n",
    "\n",
    "class CLIPViTB16Baseline(nn.Module):\n",
    "    def __init__(self, num_classes, head=\"linear\", train_backbone=False):\n",
    "        super().__init__()\n",
    "        self.backbone = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "        # freeze backbone (baseline)\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = train_backbone\n",
    "\n",
    "        hidden = self.backbone.config.hidden_size  # biasanya 768\n",
    "\n",
    "        if head == \"linear\":\n",
    "            self.classifier = nn.Linear(hidden, num_classes)\n",
    "        elif head == \"shallow\":\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(hidden, 512),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"head must be 'linear' or 'shallow'\")\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        out = self.backbone(pixel_values=pixel_values)\n",
    "        feat = out.pooler_output  # (B, hidden)\n",
    "        logits = self.classifier(feat)\n",
    "        return logits\n",
    "\n",
    "clip_model = CLIPViTB16Baseline(\n",
    "    num_classes=num_classes,\n",
    "    head=\"linear\",          # ganti \"shallow\" kalau mau MLP\n",
    "    train_backbone=False    # baseline: frozen\n",
    ").to(device)\n",
    "\n",
    "trainable = sum(p.numel() for p in clip_model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in clip_model.parameters())\n",
    "print(\"Trainable params:\", trainable)\n",
    "print(\"Total params    :\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        ce = F.cross_entropy(logits, target, weight=self.weight, reduction=\"none\")\n",
    "        pt = torch.exp(-ce)\n",
    "        fl = self.alpha * (1 - pt) ** self.gamma * ce\n",
    "        return fl.mean()\n",
    "\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0, weight=class_weights_t)\n",
    "\n",
    "import copy\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in clip_model.parameters() if p.requires_grad],\n",
    "    lr=1e-3, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "patience = 8\n",
    "best_val = float(\"inf\")\n",
    "wait = 0\n",
    "best_state = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for pixel_values, y in loader:\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = model(pixel_values)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total += y.size(0)\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "epochs = 30\n",
    "for ep in range(1, epochs + 1):\n",
    "    tr_loss, tr_acc = run_epoch(clip_model, train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(clip_model, val_loader,   train=False)\n",
    "\n",
    "    print(f\"Epoch {ep:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
    "\n",
    "    if va_loss < best_val - 1e-4:\n",
    "        best_val = va_loss\n",
    "        best_state = copy.deepcopy(clip_model.state_dict())\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    clip_model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "clip_model.eval()\n",
    "all_prob = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pixel_values, y in test_loader:\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        logits = clip_model(pixel_values)\n",
    "        prob = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        all_prob.append(prob)\n",
    "        all_true.append(y.numpy())\n",
    "\n",
    "y_prob = np.concatenate(all_prob, axis=0)\n",
    "y_true = np.concatenate(all_true, axis=0)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "prec_w = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "rec_w  = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "f1_w   = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "y_true_oh = np.eye(num_classes)[y_true]\n",
    "auc_ovr_macro = roc_auc_score(y_true_oh, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "\n",
    "print(\"\\n=== SKLEARN (MULTICLASS, ARGMAX) ===\")\n",
    "print(\"Precision (macro)   :\", prec_macro)\n",
    "print(\"Recall (macro)      :\", rec_macro)\n",
    "print(\"F1 (macro)          :\", f1_macro)\n",
    "print(\"Precision (weighted):\", prec_w)\n",
    "print(\"Recall (weighted)   :\", rec_w)\n",
    "print(\"F1 (weighted)       :\", f1_w)\n",
    "print(\"AUC (OVR macro)     :\", auc_ovr_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "def evaluate_multiclass_torch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader,\n",
    "    num_classes: int,\n",
    "    device=None,\n",
    "    criterion=None,\n",
    "    topk: int = 3,\n",
    "    average: str = \"macro\",   # \"macro\" atau \"weighted\"\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n_total = 0\n",
    "    topk_correct = 0\n",
    "\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    y_prob_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pixel_values, labels in dataloader:\n",
    "            pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True).long()\n",
    "\n",
    "            logits = model(pixel_values)              # (B, C)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            bs = labels.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            n_total += bs\n",
    "\n",
    "            probs = F.softmax(logits, dim=1)          # (B, C)\n",
    "            preds = torch.argmax(probs, dim=1)        # (B,)\n",
    "\n",
    "            # top-k accuracy\n",
    "            if topk is not None and topk > 1:\n",
    "                topk_idx = torch.topk(probs, k=topk, dim=1).indices  # (B, K)\n",
    "                topk_correct += (topk_idx == labels.unsqueeze(1)).any(dim=1).sum().item()\n",
    "\n",
    "            y_true_all.append(labels.detach().cpu().numpy())\n",
    "            y_pred_all.append(preds.detach().cpu().numpy())\n",
    "            y_prob_all.append(probs.detach().cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true_all, axis=0)\n",
    "    y_pred = np.concatenate(y_pred_all, axis=0)\n",
    "    y_prob = np.concatenate(y_prob_all, axis=0)\n",
    "\n",
    "    avg_loss = total_loss / max(n_total, 1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    # AUC multiclass OVR macro\n",
    "    # (butuh y_true one-hot)\n",
    "    auc = np.nan\n",
    "    try:\n",
    "        y_true_1hot = np.eye(num_classes, dtype=np.int32)[y_true]\n",
    "        auc = roc_auc_score(y_true_1hot, y_prob, average=\"macro\", multi_class=\"ovr\")\n",
    "    except Exception as e:\n",
    "        # kalau ada class yang tidak muncul di test set, AUC bisa gagal\n",
    "        print(f\"[WARN] AUC OVR macro tidak bisa dihitung: {e}\")\n",
    "\n",
    "    topk_acc = np.nan\n",
    "    if topk is not None and topk > 1:\n",
    "        topk_acc = topk_correct / max(n_total, 1)\n",
    "\n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"auc_ovr_macro\": auc,\n",
    "        f\"top_{topk}_acc\": topk_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "ePURn6wAwVqu",
    "outputId": "3d763b42-46a8-44c7-92ad-90d006b085e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "metrics = evaluate_multiclass_torch(\n",
    "    model=clip_model.to(device),\n",
    "    dataloader=test_loader,\n",
    "    num_classes=num_classes,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    topk=3,\n",
    "    average=\"macro\" \n",
    ")\n",
    "\n",
    "print(\"Loss       :\", metrics[\"loss\"])\n",
    "print(\"Accuracy   :\", metrics[\"accuracy\"])\n",
    "print(\"Precision  :\", metrics[\"precision\"])\n",
    "print(\"Recall     :\", metrics[\"recall\"])\n",
    "print(\"AUC        :\", metrics[\"auc_ovr_macro\"])\n",
    "print(\"Top K        :\", metrics[\"top_3_acc\"])\n",
    "print(\"F1-Score   :\", metrics[\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hitung jumlah class\n",
    "classes, counts = np.unique(y_test, axis=0, return_counts=True)\n",
    "\n",
    "# Print hasilnya\n",
    "print(\"Kelas: \", classes)\n",
    "print(\"Jumlah: \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB6r9S0Yoicx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "# Function to plot confusion matrix    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": clip_model.state_dict(),\n",
    "        \"num_classes\": num_classes,\n",
    "        \"clip_name\": \"openai/clip-vit-base-patch16\",\n",
    "        \"head\": \"linear\",\n",
    "        \"train_backbone\": False,\n",
    "    },\n",
    "    \"model-h5/baseline_clip_vitb16_linearhead.pt\"\n",
    ")\n",
    "print(\"Saved: baseline_clip_vitb16_linearhead.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Keep dataset paths and output paths configurable for reproducibility.\n",
    "- If you publish this notebook, ensure no private paths or secrets are embedded.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOvWgVEKhpylpZviJkoIQr0",
   "name": "Resnet_skin_lesions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
